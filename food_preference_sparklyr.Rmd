---
title: "R Notebook"
output: html_notebook
---

This is an [R Markdown](http://rmarkdown.rstudio.com) Notebook. When you execute code within the notebook, the results appear beneath the code. 

Try executing this chunk by clicking the *Run* button within the chunk or by placing your cursor inside it and pressing *Cmd+Shift+Enter*. 

```{r}
plot(cars)
```

Add a new chunk by clicking the *Insert Chunk* button on the toolbar or by pressing *Cmd+Option+I*.

When you save the notebook, an HTML file containing the code and output will be saved alongside it (click the *Preview* button or press *Cmd+Shift+K* to preview the HTML file).

**Predicting Food preferences with sparklyr (Machine Learning)**

*Shirin play ground*

Spark's distributed machine learning library called *MLlib* sits on top of the spark core framework. With Sparklyr, you can easily access the MLlib. 
```{r}
# Whether a preference for a country's cuisine can be predicted based on preference of other countries cuisines
library(sparklyr)

# install spark locally
spark_install(version = "2.0.0")
```

Now we can connect to a local spark instance

```{r}
library(sparklyr)
sc=spark_connect(master = "local",version = "2.0.0")
```

Preparations for a custom ggplot theme

Load packages

```{r}
library(tidyr)
library(ggplot2)
library(dplyr)
library(ggrepel) # for non overlapping texy labels in plots

mytheme=function(base_size=12,base_family="sans"){
  theme_minimal(base_size = base_size,base_family = base_family)+
    theme(
      axis.text = element_text(size=12),
      axis.title = element_text(size=14),
      panel.grid.major = element_line(color="grey"),
      panel.grid.minor = element_blank(),
      panel.background = element_rect(fill = "aliceblue"),
      strip.background = element_rect(fill="lightgrey",color = "grey",size=1),
      strip.text = element_text(face="bold",size=12,color="black"),
      legend.position = "right",
      legend.justification = "top",
      panel.border = element_rect(color="grey",fill=NA,size = 0.85)
    )
}
```

all NA are recoded as 0

```{r}
# load the 538 library
library(fivethirtyeight)
head(food_world_cup)
food_world_cup[food_world_cup=="N/A"]=NA
food_world_cup[,9:48][is.na(food_world_cup[,9:48])]=0
food_world_cup$gender=as.factor(food_world_cup$gender)
food_world_cup$location=as.factor(food_world_cup$location)

```

We calculate the percentage for each preference category and plot them as a pie chart facetted by country

```{r}
# calculating percentages by country
head(food_world_cup)
percentages=food_world_cup %>% 
  select(algeria:vietnam) %>% 
  gather(x,y) %>% 
  group_by(x,y) %>% 
  summarise(n=n()) %>% 
  mutate(Percent=round(n/sum(n)*100,2))
# rename countries and plot

percentages %>% 
  mutate(x_2=gsub("_"," ",x)) %>% 
  mutate(x_2=gsub("(^|[[:space:]])([[:alpha:]])","\\1\\U\\2",x_2,perl=TRUE)) %>% 
  mutate(x_2=gsub("And","and",x_2)) %>% 
  ggplot(aes(x="",y=Percent,fill=y))+
  geom_bar(width = 1,stat="identity")+
  theme_minimal()+
  coord_polar("y",start = 0)+
  facet_wrap(~x_2,ncol=8)+
  labs(fill="")
```

Imputing missing values, install package mice

```{r}
library(mice)
dataset_impute=mice(food_world_cup[,-c(1,2)],print=FALSE) # imputes all missing values except in columns 1,2

# replace the food world cup dataset with the imputations

food_world_cup=cbind(food_world_cup[,2,drop=FALSE],mice::complete(dataset_impute,1))

```

Transforming preference variable- perference (points awarded) is categorical but using them as factor levels make models more complex - so they are converted into numbers and transforming them by dividing through the mean of the non zero values for each country.

```{r}
str(food_world_cup[,8:47])
# convert all of these to numbers

food_world_cup[8:47]=lapply(food_world_cup[8:47],FUN = as.numeric)
countries=paste(colnames(food_world_cup)[-c(1:7)])

for (countryname in countries){
  food_world_cup[paste(countryname,"trans",sep = "_")]=food_world_cup[countryname]/mean(food_world_cup[food_world_cup[countryname]>0,countryname])
}
```

We now plot density curves to see the distributions of transformed countries

```{r}
food_world_cup %>% 
  gather(x,y,algeria_trans:vietnam_trans) %>% 
  mutate(x_2=gsub("_trans","",x)) %>% 
  mutate(x_2=gsub("_"," ",x_2)) %>% 
  mutate(x_2=gsub("(^|[[:space:]])([[:alpha:]])","\\1\\U\\2",x_2,perl=TRUE)) %>% 
  mutate(x_2=gsub("And","and",x_2)) %>% 
  ggplot(aes(y))+
  geom_density(fill="navy",alpha=0.7)+
  mytheme()+
  facet_wrap(~x_2,ncol = 8)+
  labs(x="transformed preference")
```

Next we want to see which countries are most like and whether there is a gender bias for cuisines for certain countries

```{r}
food_world_cup_gather=food_world_cup %>% 
  collect() %>% 
  gather(country,value,algeria:vietnam)
food_world_cup_gather$country=as.factor(food_world_cup_gather$country)
food_world_cup_gather$value=as.numeric(food_world_cup_gather$value)

# most liked cuisines

food_world_cup_gather %>% 
  select(country,value) %>% 
  group_by(country) %>% 
  mutate(average_value=mean(value)) %>% 
  arrange(desc(average_value)) %>% 
  ggplot(aes(x=reorder(country,average_value),y=average_value,fill=average_value))+
  geom_bar(stat="identity")+
  mytheme()+
  theme(axis.text.x = element_text(angle=90,vjust = 0.5,hjust = 1))
```

One last data manipulation before machine learning, finding the difference between the genders

```{r}
food_world_cup %>% 
  collect() %>% 
  mutate_each_(funs(as.numeric),countries) %>% # note the mutate_each_ (- after each)
  group_by(gender) %>% 
  summarise_each_(funs(mean),countries) %>%  # _ after each means to summarize by the given variable, i.e countries
  summarise_each_(funs(diff),countries) %>% # computes the diff between the two categories- genders
  gather(x,y) %>% 
  ggplot(aes(x,y,fill=sign(y)))+
  geom_bar(stat="identity",alpha=0.7)+
  mytheme()+
  theme(axis.text.x = element_text(angle=90,vjust=0.5,hjust=1))+
  labs(x="",y="difference between genders")
  
```

As gender is very close to zero, means men and women have similar food preferences

**Spark**

Let's copy the data to a spark instance

```{r}
food_world_cup=copy_to(sc,food_world_cup)
```

**Principal Component Analysis (PCA)**

One of the functions of Sparks MLlib is PCA (ml_pca()- we'll use it to find which countries fall on a 2 dimensional plane of the first two principal components

```{r}
pca=food_world_cup %>% 
  mutate_each_(funs(as.numeric),countries) %>% 
  ml_pca(features = paste(colnames(food_world_cup)[-c(1:47)]))

head(pca)
# plot the pca 
library(tibble)
as.data.frame(pca$components) %>% 
  rownames_to_column(var="lables") %>% 
  mutate(x_2=gsub("_trans","",lables)) %>% 
  ggplot(aes(x=PC1,y=PC2,color=x_2,label=x_2))+
  geom_point(size=2,alpha=0.6)+
  geom_text_repel()+
  labs(x=paste0("PC1: ",round(pca$explained.variance[1],2)*100,"% variance"),
       y=paste0("PC2: ",round(pca$explained.variance[2],2)*100,"% variance"))+
  mytheme()+
  guides(fill=FALSE,color=FALSE)


```

The least well known countries cluster on top right while most liked countries are on bottom right

Finally, *Preparing the data for machine learning*

first, convert the factor strings of the non country features to indexes. We use the ft_sting_indexer() function of sparklyr


```{r}
colnames(food_world_cup)
food_world_cup=tbl(sc,"food_world_cup") %>% 
  ft_string_indexer(input.col = "interest",output.col = "interest_idx") %>% 
  ft_string_indexer(input.col = "gender",output.col = "gender_idx") %>% 
  ft_string_indexer(input.col = "age",output.col = "age_idx") %>% 
  ft_string_indexer(input.col = "household_income",output.col = "household_income_idx") %>% 
  ft_string_indexer(input.col ="education",output.col = "eduction_idx" ) %>% 
  ft_string_indexer(input.col = "location",output.col = "location_idx") %>% 
  ft_string_indexer(input.col = "knowledge",output.col = "knowledge_idx")
```

Divide the data into training and test set

```{r}
partitions=food_world_cup %>% 
  sdf_partition(training=0.75,test=0.25,seed=123)
```

**Machine Learning Modeling**

We run the random forest algorithm to predict each country preference based on other country preference and demographic information

For each country (response variable), we define the features of other countries transformed values and indexed factor variables

then filtering out data rows where response variable was 0

Using ml_random_forest() function to run classification models. Initially run models on all features, then extract 10 features with highest importance and rerun the model on the subset of features

The sdf_predict() function is used to predict the classes of the test set

To obtain quality metrics F1, weighted precision and weighted recall, we need to copy the prediction table to spark's instance and run the ml_classification_eval() function

Finally, combining the output tables of all countries to compare

```{r}
require(lazyeval)

for (countryname in countries) {
  features=colnames(partitions$training)[-grep(countryname,colnames(partitions$training))] # get features corresponding to the selected countryname
} 
  features=features[grep("_trans|_idx",features)] # only keep features with either suffix trans or idx

for (countryname in countries) {  
  fit=partitions$training %>% 
    filter_(interp(~var>0,var=as.name(countryname))) %>% # filter out all features with values <=0
    ml_random_forest(intercept=FALSE,response=countryname,features=features,type="classification")
  
}
  summary(fit)
  feature_imp=ml_tree_feature_importance(sc,fit)
  ggplot(as.data.frame(feature_imp),aes(x=feature,y=importance))+geom_bar(stat="identity")+
    coord_flip()
  # features=as.character(feature_imp[1:10,2]) # now use only the top 10 features for training the model
  
  # fit=partitions$training %>% 
  #   filter_(interp(~var>0,var=as.name(countryname))) %>% 
  #   ml_random_forest(intercept=FALSE,response=countryname,features=features,type="classification")
  # partitions$test=partitions$test %>% 
  #   filter_(interp(~var>0,var=as.name(countryname)))
 # Now predict using the developed model - fit
   pred=sdf_predict(fit,partitions$test) %>% 
    collect()
  pred
  pred_2=as.data.frame(table(pred[[countryname]],pred$prediction))
    

```

